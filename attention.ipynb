{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwOPmL/Zm0fIrMRWh8MiQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4may/Transformer/blob/master/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuXH2Q59dD60",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "2018年にGoogle Brainから発表された新しいNNアーキテクチャ。attentionを使った構造が肝になる。\n",
        "\n",
        "RNNと比較した時の、attentionのメリットは以下の通り。\n",
        "\n",
        "* 並列計算可能であるため、GPUの恩恵を受けやすい\n",
        "* データの時空間関係に依存せず、離れた位置にある情報同士の関係性も表現できる\n",
        "* RNNのようなループ構造をもたず、構造がシンプル\n",
        "\n",
        "逆に、デメリットは以下の通り。\n",
        "\n",
        "* 時系列データの場合、ある時刻tの出力を計算をするのに全ての時刻の入力を必要とするため、非効率な場合がある\n",
        "\n",
        "## Attention\n",
        "\n",
        "Transformerを学ぶ前に、先にAttentionを学んでおこう。ここでは、attentionを使って日本語を英語に翻訳してみよう。\n",
        "\n",
        "[Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
        "\n",
        "### 必要なモジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtsFeNggQLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzrXlQq4gwHb",
        "colab_type": "text"
      },
      "source": [
        "### データの準備\n",
        "\n",
        "データの前処理として、以下のことをしよう。\n",
        "\n",
        "1. 各文にstart/end tokenを付け加える。\n",
        "2. 各文から特殊文字を取り除く。\n",
        "3. 単語と、単語のindexの変換辞書を作る\n",
        "4. 各文をpaddingする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoSZuOR9tOWX",
        "colab_type": "text"
      },
      "source": [
        "### 形態素解析\n",
        "\n",
        "翻訳辞書内の日本語は分かち書きされていないため、形態素解析が必要。\n",
        "\n",
        "google colab上に、辞書と必要なモジュールをインストールする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgD03fFumUk",
        "colab_type": "code",
        "outputId": "9f215122-aba0-47bd-bbe8-b50cb4d5f45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30 swig swig3.0\n",
            "0 upgraded, 23 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 4,978 kB of archives.\n",
            "After this operation, 21.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 4,978 kB in 1s (3,692 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../21-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../22-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "0 packages upgraded, 11 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.3 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.3 [68.7 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.3 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 2s (14.7 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 145818 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 34.9MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3ICbYiuvYd",
        "colab_type": "code",
        "outputId": "3145f3e7-67b6-428b-cf4f-74ef09cad333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mecab-ipadic-neologd'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 75 (delta 5), reused 54 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), done.\n",
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : Check the existance of libraries\n",
            "[install-mecab-ipadic-NEologd] :     find => ok\n",
            "[install-mecab-ipadic-NEologd] :     sort => ok\n",
            "[install-mecab-ipadic-NEologd] :     head => ok\n",
            "[install-mecab-ipadic-NEologd] :     cut => ok\n",
            "[install-mecab-ipadic-NEologd] :     egrep => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab-config => ok\n",
            "[install-mecab-ipadic-NEologd] :     make => ok\n",
            "[install-mecab-ipadic-NEologd] :     curl => ok\n",
            "[install-mecab-ipadic-NEologd] :     sed => ok\n",
            "[install-mecab-ipadic-NEologd] :     cat => ok\n",
            "[install-mecab-ipadic-NEologd] :     diff => ok\n",
            "[install-mecab-ipadic-NEologd] :     tar => ok\n",
            "[install-mecab-ipadic-NEologd] :     unxz => ok\n",
            "[install-mecab-ipadic-NEologd] :     xargs => ok\n",
            "[install-mecab-ipadic-NEologd] :     grep => ok\n",
            "[install-mecab-ipadic-NEologd] :     iconv => ok\n",
            "[install-mecab-ipadic-NEologd] :     patch => ok\n",
            "[install-mecab-ipadic-NEologd] :     which => ok\n",
            "[install-mecab-ipadic-NEologd] :     file => ok\n",
            "[install-mecab-ipadic-NEologd] :     openssl => ok\n",
            "[install-mecab-ipadic-NEologd] :     awk => ok\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd is already up-to-date\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd will be install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Make mecab-ipadic-NEologd\n",
            "[make-mecab-ipadic-NEologd] : Start..\n",
            "[make-mecab-ipadic-NEologd] : Check local seed directory\n",
            "[make-mecab-ipadic-NEologd] : Check local seed file\n",
            "[make-mecab-ipadic-NEologd] : Check local build directory\n",
            "[make-mecab-ipadic-NEologd] : create /content/mecab-ipadic-neologd/libexec/../build\n",
            "[make-mecab-ipadic-NEologd] : Download original mecab-ipadic file\n",
            "[make-mecab-ipadic-NEologd] : Try to access to https://ja.osdn.net\n",
            "[make-mecab-ipadic-NEologd] : Try to download from https://ja.osdn.net/frs/g_redir.php?m=kent&f=mecab%2Fmecab-ipadic%2F2.7.0-20070801%2Fmecab-ipadic-2.7.0-20070801.tar.gz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 11.6M  100 11.6M    0     0  1815k      0  0:00:06  0:00:06 --:--:-- 2555k\n",
            "Hash value of /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801.tar.gz matched\n",
            "[make-mecab-ipadic-NEologd] : Decompress original mecab-ipadic file\n",
            "mecab-ipadic-2.7.0-20070801/\n",
            "mecab-ipadic-2.7.0-20070801/README\n",
            "mecab-ipadic-2.7.0-20070801/AUTHORS\n",
            "mecab-ipadic-2.7.0-20070801/COPYING\n",
            "mecab-ipadic-2.7.0-20070801/ChangeLog\n",
            "mecab-ipadic-2.7.0-20070801/INSTALL\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.am\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.in\n",
            "mecab-ipadic-2.7.0-20070801/NEWS\n",
            "mecab-ipadic-2.7.0-20070801/aclocal.m4\n",
            "mecab-ipadic-2.7.0-20070801/config.guess\n",
            "mecab-ipadic-2.7.0-20070801/config.sub\n",
            "mecab-ipadic-2.7.0-20070801/configure\n",
            "mecab-ipadic-2.7.0-20070801/configure.in\n",
            "mecab-ipadic-2.7.0-20070801/install-sh\n",
            "mecab-ipadic-2.7.0-20070801/missing\n",
            "mecab-ipadic-2.7.0-20070801/mkinstalldirs\n",
            "mecab-ipadic-2.7.0-20070801/Adj.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adnominal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adverb.csv\n",
            "mecab-ipadic-2.7.0-20070801/Auxil.csv\n",
            "mecab-ipadic-2.7.0-20070801/Conjunction.csv\n",
            "mecab-ipadic-2.7.0-20070801/Filler.csv\n",
            "mecab-ipadic-2.7.0-20070801/Interjection.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adjv.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.demonst.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.nai.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.name.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.number.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.org.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.place.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.proper.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.verbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp-col.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp.csv\n",
            "mecab-ipadic-2.7.0-20070801/Prefix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Suffix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Symbol.csv\n",
            "mecab-ipadic-2.7.0-20070801/Verb.csv\n",
            "mecab-ipadic-2.7.0-20070801/char.def\n",
            "mecab-ipadic-2.7.0-20070801/feature.def\n",
            "mecab-ipadic-2.7.0-20070801/left-id.def\n",
            "mecab-ipadic-2.7.0-20070801/matrix.def\n",
            "mecab-ipadic-2.7.0-20070801/pos-id.def\n",
            "mecab-ipadic-2.7.0-20070801/rewrite.def\n",
            "mecab-ipadic-2.7.0-20070801/right-id.def\n",
            "mecab-ipadic-2.7.0-20070801/unk.def\n",
            "mecab-ipadic-2.7.0-20070801/dicrc\n",
            "mecab-ipadic-2.7.0-20070801/RESULT\n",
            "[make-mecab-ipadic-NEologd] : Configure custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200315\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for working aclocal-1.4... missing\n",
            "checking for working autoconf... missing\n",
            "checking for working automake-1.4... missing\n",
            "checking for working autoheader... missing\n",
            "checking for working makeinfo... missing\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "[make-mecab-ipadic-NEologd] : Encode the character encoding of system dictionary resources from EUC_JP to UTF-8\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.others.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Filler.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.nai.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Auxil.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Suffix.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adjv.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Verb.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adnominal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.number.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Prefix.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp-col.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.demonst.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.org.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.verbal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Interjection.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adj.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.name.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Others.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.proper.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.place.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Conjunction.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adverbal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adverb.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Symbol.csv \n",
            "rm ./Noun.others.csv \n",
            "rm ./Filler.csv \n",
            "rm ./Noun.nai.csv \n",
            "rm ./Auxil.csv \n",
            "rm ./Noun.csv \n",
            "rm ./Suffix.csv \n",
            "rm ./Noun.adjv.csv \n",
            "rm ./Verb.csv \n",
            "rm ./Postp.csv \n",
            "rm ./Adnominal.csv \n",
            "rm ./Noun.number.csv \n",
            "rm ./Prefix.csv \n",
            "rm ./Postp-col.csv \n",
            "rm ./Noun.demonst.csv \n",
            "rm ./Noun.org.csv \n",
            "rm ./Noun.verbal.csv \n",
            "rm ./Interjection.csv \n",
            "rm ./Adj.csv \n",
            "rm ./Noun.name.csv \n",
            "rm ./Others.csv \n",
            "rm ./Noun.proper.csv \n",
            "rm ./Noun.place.csv \n",
            "rm ./Conjunction.csv \n",
            "rm ./Noun.adverbal.csv \n",
            "rm ./Adverb.csv \n",
            "rm ./Symbol.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./matrix.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./unk.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./left-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./pos-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./rewrite.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./feature.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./right-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./char.def \n",
            "rm ./matrix.def \n",
            "rm ./unk.def \n",
            "rm ./left-id.def \n",
            "rm ./pos-id.def \n",
            "rm ./rewrite.def \n",
            "rm ./feature.def \n",
            "rm ./right-id.def \n",
            "rm ./char.def \n",
            "mv ./Adnominal.csv.utf8 ./Adnominal.csv \n",
            "mv ./Suffix.csv.utf8 ./Suffix.csv \n",
            "mv ./Others.csv.utf8 ./Others.csv \n",
            "mv ./Conjunction.csv.utf8 ./Conjunction.csv \n",
            "mv ./Verb.csv.utf8 ./Verb.csv \n",
            "mv ./Noun.others.csv.utf8 ./Noun.others.csv \n",
            "mv ./Adj.csv.utf8 ./Adj.csv \n",
            "mv ./Noun.number.csv.utf8 ./Noun.number.csv \n",
            "mv ./Auxil.csv.utf8 ./Auxil.csv \n",
            "mv ./Noun.place.csv.utf8 ./Noun.place.csv \n",
            "mv ./Filler.csv.utf8 ./Filler.csv \n",
            "mv ./Noun.verbal.csv.utf8 ./Noun.verbal.csv \n",
            "mv ./Postp-col.csv.utf8 ./Postp-col.csv \n",
            "mv ./Noun.csv.utf8 ./Noun.csv \n",
            "mv ./feature.def.utf8 ./feature.def \n",
            "mv ./Adverb.csv.utf8 ./Adverb.csv \n",
            "mv ./Noun.adverbal.csv.utf8 ./Noun.adverbal.csv \n",
            "mv ./Noun.proper.csv.utf8 ./Noun.proper.csv \n",
            "mv ./Noun.demonst.csv.utf8 ./Noun.demonst.csv \n",
            "mv ./Symbol.csv.utf8 ./Symbol.csv \n",
            "mv ./matrix.def.utf8 ./matrix.def \n",
            "mv ./pos-id.def.utf8 ./pos-id.def \n",
            "mv ./Noun.org.csv.utf8 ./Noun.org.csv \n",
            "mv ./rewrite.def.utf8 ./rewrite.def \n",
            "mv ./unk.def.utf8 ./unk.def \n",
            "mv ./Noun.name.csv.utf8 ./Noun.name.csv \n",
            "mv ./Noun.nai.csv.utf8 ./Noun.nai.csv \n",
            "mv ./Interjection.csv.utf8 ./Interjection.csv \n",
            "mv ./Prefix.csv.utf8 ./Prefix.csv \n",
            "mv ./right-id.def.utf8 ./right-id.def \n",
            "mv ./char.def.utf8 ./char.def \n",
            "mv ./Postp.csv.utf8 ./Postp.csv \n",
            "mv ./Noun.adjv.csv.utf8 ./Noun.adjv.csv \n",
            "mv ./left-id.def.utf8 ./left-id.def \n",
            "[make-mecab-ipadic-NEologd] : Fix yomigana field of IPA dictionary\n",
            "patching file Noun.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Verb.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.adverbal.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.others.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Prefix.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.demonst.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "[make-mecab-ipadic-NEologd] : Copy user dictionary resource\n",
            "[make-mecab-ipadic-NEologd] : Install adverb entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adverb-dict-seed.20150623.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install interjection entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-interjection-dict-seed.20170216.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-common-noun-ortho-variant-dict-seed.20170228.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-proper-noun-ortho-variant-dict-seed.20161110.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install entries of orthographic variant of a noun used as verb form using /content/mecab-ipadic-neologd/libexec/../seed/neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install frequent adjective orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-std-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent adjective orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-exp-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install adjective verb orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-verb-dict-seed.20160324.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent datetime representation entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-date-time-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent quantity representation entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-quantity-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install entries of ill formed words using /content/mecab-ipadic-neologd/libexec/../seed/neologd-ill-formed-words-dict-seed.20170127.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Re-Index system dictionary\n",
            "reading ./unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "reading ./neologd-proper-noun-ortho-variant-dict-seed.20161110.csv ... 138379\n",
            "reading ./Noun.others.csv ... 153\n",
            "reading ./neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv ... 26058\n",
            "reading ./neologd-interjection-dict-seed.20170216.csv ... 4701\n",
            "reading ./Filler.csv ... 19\n",
            "reading ./Noun.nai.csv ... 42\n",
            "reading ./Auxil.csv ... 199\n",
            "reading ./Noun.csv ... 60734\n",
            "reading ./neologd-common-noun-ortho-variant-dict-seed.20170228.csv ... 152869\n",
            "reading ./Suffix.csv ... 1448\n",
            "reading ./Noun.adjv.csv ... 3328\n",
            "reading ./Verb.csv ... 130750\n",
            "reading ./Postp.csv ... 146\n",
            "reading ./neologd-adjective-exp-dict-seed.20151126.csv ... 1051146\n",
            "reading ./Adnominal.csv ... 135\n",
            "reading ./Noun.number.csv ... 42\n",
            "reading ./neologd-ill-formed-words-dict-seed.20170127.csv ... 60616\n",
            "reading ./Prefix.csv ... 224\n",
            "reading ./neologd-quantity-infreq-dict-seed.20190415.csv ... 229216\n",
            "reading ./neologd-date-time-infreq-dict-seed.20190415.csv ... 16866\n",
            "reading ./Postp-col.csv ... 91\n",
            "reading ./mecab-user-dict-seed.20200315.csv ... 3187380\n",
            "reading ./Noun.demonst.csv ... 120\n",
            "reading ./Noun.org.csv ... 17149\n",
            "reading ./neologd-adverb-dict-seed.20150623.csv ... 139792\n",
            "reading ./neologd-adjective-verb-dict-seed.20160324.csv ... 20268\n",
            "reading ./Noun.verbal.csv ... 12150\n",
            "reading ./Interjection.csv ... 252\n",
            "reading ./Adj.csv ... 27210\n",
            "reading ./Noun.name.csv ... 34215\n",
            "reading ./Others.csv ... 2\n",
            "reading ./Noun.proper.csv ... 27493\n",
            "reading ./Noun.place.csv ... 73194\n",
            "reading ./Conjunction.csv ... 171\n",
            "reading ./Noun.adverbal.csv ... 808\n",
            "reading ./Adverb.csv ... 3032\n",
            "tcmalloc: large alloc 1308631040 bytes == 0x56305b82c000 @  0x7facf506a887 0x7facf41d9b8b 0x7facf41db133 0x7facf4cdfa66 0x7facf4cb3e53 0x7facf4899b97 0x563002bde67a\n",
            "reading ./neologd-adjective-std-dict-seed.20151126.csv ... 507812\n",
            "reading ./Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "[make-mecab-ipadic-NEologd] : Make custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200315\n",
            "make: Nothing to be done for 'all'.\n",
            "[make-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : Get results of tokenize test\n",
            "[test-mecab-ipadic-NEologd] : Start..\n",
            "[test-mecab-ipadic-NEologd] : Replace timestamp from 'git clone' date to 'git commit' date\n",
            "[test-mecab-ipadic-NEologd] : Get buzz phrases\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1736  100  1736    0     0   1850      0 --:--:-- --:--:-- --:--:--  1850\n",
            "[test-mecab-ipadic-NEologd] : Get difference between default system dictionary and mecab-ipadic-NEologd\n",
            "[test-mecab-ipadic-NEologd] : Tokenize phrase using default system dictionary\n",
            "[test-mecab-ipadic-NEologd] : Tokenize phrase using mecab-ipadic-NEologd\n",
            "[test-mecab-ipadic-NEologd] : Get result of diff\n",
            "[test-mecab-ipadic-NEologd] : Please check difference between default system dictionary and mecab-ipadic-NEologd\n",
            "\n",
            "default system dictionary\t  |\tmecab-ipadic-NEologd\n",
            "矢部 浩之 \t\t\t  |\t矢部浩之 \n",
            "すま たん \t\t\t  |\tすまたん \n",
            "柴 咲 コウ \t\t\t  |\t柴咲コウ \n",
            "岡村 隆史 ANN \t\t\t  |\t岡村隆史 ANN \n",
            "鞘 師 \t\t\t\t  |\t鞘師 \n",
            "や べっち \t\t\t  |\tやべっち \n",
            "久保 ミツ ロウ \t\t\t  |\t久保ミツロウ \n",
            "ナインティナイン 岡村 隆史 の \t  |\tナインティナイン岡村隆史のオー\n",
            "り ほり ほ \t\t\t  |\tりほりほ \n",
            "ナイ ナイ \t\t\t  |\tナイナイ \n",
            "邪 ンヌ \t\t\t  |\t邪ンヌ \n",
            "\n",
            "[test-mecab-ipadic-NEologd] : Finish..\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Please check the list of differences in the upper part.\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Do you want to install mecab-ipadic-NEologd? Type yes or no.\n",
            "[install-mecab-ipadic-NEologd] : OK. Let's install mecab-ipadic-NEologd.\n",
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : /usr/lib/x86_64-linux-gnu/mecab/dic isn't current user's directory\n",
            "[install-mecab-ipadic-NEologd] : Sudo make install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "make[1]: Entering directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200315'\n",
            "make[1]: Nothing to be done for 'install-exec-am'.\n",
            "/bin/bash ./mkinstalldirs /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab/dic\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            " /usr/bin/install -c -m 644 ./matrix.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/matrix.bin\n",
            " /usr/bin/install -c -m 644 ./char.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/char.bin\n",
            " /usr/bin/install -c -m 644 ./sys.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/sys.dic\n",
            " /usr/bin/install -c -m 644 ./unk.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/unk.dic\n",
            " /usr/bin/install -c -m 644 ./left-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/left-id.def\n",
            " /usr/bin/install -c -m 644 ./right-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/right-id.def\n",
            " /usr/bin/install -c -m 644 ./rewrite.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/rewrite.def\n",
            " /usr/bin/install -c -m 644 ./pos-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/pos-id.def\n",
            " /usr/bin/install -c -m 644 ./dicrc /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/dicrc\n",
            "make[1]: Leaving directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200315'\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Install completed.\n",
            "[install-mecab-ipadic-NEologd] : When you use MeCab, you can set '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd' as a value of '-d' option of MeCab.\n",
            "[install-mecab-ipadic-NEologd] : Usage of mecab-ipadic-NEologd is here.\n",
            "Usage:\n",
            "    $ mecab -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd ...\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKlOPxT2AiVy",
        "colab_type": "text"
      },
      "source": [
        "動作確認してみよう。print(\"input Language: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRE4aaTMva5n",
        "colab_type": "code",
        "outputId": "38dd6acc-77e2-4200-b04c-792f8d5eadc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import MeCab\n",
        "import subprocess\n",
        "\n",
        "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
        "sub = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "path = (sub.communicate()[0]).decode('utf-8')\n",
        "m = MeCab.Tagger(\"-d {0}\".format(path))\n",
        "test = m.parse('今日は、良い気分になれる日だ！')\n",
        "print(test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "良い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,良い,ヨイ,ヨイ\n",
            "気分\t名詞,一般,*,*,*,*,気分,キブン,キブン\n",
            "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
            "なれる\t動詞,自立,*,*,一段,基本形,なれる,ナレル,ナレル\n",
            "日\t名詞,非自立,副詞可能,*,*,*,日,ヒ,ヒ\n",
            "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "EOS\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fF1ZlUExSFF",
        "colab_type": "code",
        "outputId": "61220c29-3c1e-4d6b-be86-154e4cc138de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lines = m.parse('今日は、良い気分になれる日だ！').split('\\n')\n",
        "#EOF, 空文字はスキップ\n",
        "tokens = [line.split('\\t')[0] for line in lines[:-2]]\n",
        "tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['今日', 'は', '、', '良い', '気分', 'に', 'なれる', '日', 'だ', '！']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQifv_Tf3MjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mecab_tokenize(text):\n",
        "  lines = m.parse(text).split('\\n')\n",
        "  tokens = [line.split('\\t')[0] for line in lines[:-2]]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvPbQCH56FkE",
        "colab_type": "text"
      },
      "source": [
        "これで、分かち書きの準備ができた。\n",
        "\n",
        "#### 翻訳辞書をダウンロード\n",
        "\n",
        "[辞書のDL先](http://www.manythings.org/anki/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5MF6bjcqr4Q",
        "colab_type": "code",
        "outputId": "956b5a93-a6ce-4c52-b53c-87842edda8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#ローカル辞書をアップロードする\n",
        "!unzip jpn-eng.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  jpn-eng.zip\n",
            "  inflating: jpn.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDA8ZkCVk6CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = 'jpn.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsIOlPcwm2Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  #日本語の場合、ユニコードカテゴリMnに該当する文字もnormalizeする必要がある。\n",
        "  #そうでなければ、濁点や丸点まで消えてしまう。\n",
        "  #return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s))\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  #形態素解析\n",
        "  tokens = mecab_tokenize(w)\n",
        "  w = ' '.join(tokens)\n",
        "\n",
        "  #小文字に変え、かつ文の先頭と末尾の空白と改行を削除する。\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  #単語と句読点との間にスペースを作る\n",
        "  #彼 は、少年 です。 -> 彼 は 、 少年 です 。\n",
        "  w = re.sub(r\"([?.!,、。])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  #<start> token、<end> tokenを付け加える\n",
        "  #モデルの予測の開始場所と終了場所がわかるようにする。\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vgOXC1Uo4WH",
        "colab_type": "code",
        "outputId": "6ee13474-0812-4f0d-9f17-a5c9e480de7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "jp_sentence = u\"この本を、借りてもいいですか?\"\n",
        "preprocess_sentence(en_sentence)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> may i borrow this book ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owpfFWBFpKZZ",
        "colab_type": "code",
        "outputId": "ca7f387d-73ee-48ce-925a-5dc7082453a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preprocess_sentence(jp_sentence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> この 本 を 、 借りて も いい です か ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXCPwsgzpqUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  # 翻訳辞書の中身は以下のようになっている。\n",
        "  # 最後の注釈はいらないので、落とす。\n",
        "  # 英語\\t日本語\\t注釈\\n\n",
        "  # Go.\t行け。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421985 (Ninja)\n",
        "  # Go.\t行きなさい。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421986 (Ninja)\n",
        "  # Hi.\tこんにちは。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #373351 (tommy_san)\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:-1]] for l in lines[:num_examples]]\n",
        "  \n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpp_Qi2hqVvG",
        "colab_type": "code",
        "outputId": "7619e69d-098d-4bbb-838b-320094046c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "en, jp = create_dataset(path_to_file, None)\n",
        "en[-1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> i do many things at the same time , so not only am i reading things by akutagawa , i've also increased the amount of time i spend reading in english and i also read a little in german every day . <end>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjGSDQBsyrw",
        "colab_type": "code",
        "outputId": "255f6528-ee2d-48b3-fa27-a074cd90d68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "jp[-1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> 色々 並行 し て やっ てる から 芥川 ばかり 読ん でる の で も ない の だ よ 。 今 は 英語 読ん でる 時間 が 増え てる 。 ドイツ語 も 毎日 少し ずつ やっ てる 。 <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIBvLcVz7mRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7do-AjJ58BIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  tartget_lang, input_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "  target_tensor, tartget_lang_tokenizer = tokenize(tartget_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, input_lang_tokenizer, tartget_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "068HEzi78bMK",
        "colab_type": "text"
      },
      "source": [
        "## 実験の高速化のため、データセットのサイズに制限を設ける\n",
        "\n",
        "100,000データだと多いので、30,000にしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6elHbSi8oYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "データ数を絞る\n",
        "'''\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, input_lang, target_lang = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4KCSnQJ9RRd",
        "colab_type": "code",
        "outputId": "28208c88-caee-4a17-9193-3815bdb7e589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "target tensorの最大長を取得\n",
        "'''\n",
        "max_length_target = target_tensor.shape[1]\n",
        "max_length_input = input_tensor.shape[1]\n",
        "max_length_target"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To4fqDMf9Wqm",
        "colab_type": "text"
      },
      "source": [
        "trainingデータとvalidationデータを8:2の割合で分けてみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OonyimFk9bsy",
        "colab_type": "code",
        "outputId": "7b5739b6-524d-409d-d0a3-cd8f4064ccc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_tensor_train, input_tensor_valid, target_tensor_train, target_tensor_valid = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_valid), len(target_tensor_valid))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJi2IHI0-B77",
        "colab_type": "text"
      },
      "source": [
        "次に、単語と単語インデックスのマッピング関数を定義しよう。Attention layerは文字列ではなく数値を入力にとるため、文字列の数値変換が必要になる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDkF1fRp-NZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(\"%d ------> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOVg0Kg-ZaF",
        "colab_type": "code",
        "outputId": "9b35d38b-c818-47f8-dddb-441af2a40a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(\"Input Language: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language: index to word mapping\")\n",
        "convert(target_lang, target_tensor_train[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language: index to word mapping\n",
            "1 ------> <start>\n",
            "39 ------> あなた\n",
            "4 ------> は\n",
            "732 ------> お母さん\n",
            "7 ------> に\n",
            "845 ------> そっくり\n",
            "46 ------> ね\n",
            "3 ------> 。\n",
            "2 ------> <end>\n",
            "\n",
            "Target Language: index to word mapping\n",
            "1 ------> <start>\n",
            "7 ------> you\n",
            "143 ------> look\n",
            "85 ------> just\n",
            "38 ------> like\n",
            "34 ------> your\n",
            "172 ------> mother\n",
            "3 ------> .\n",
            "2 ------> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s27Qwkaa_Mnf",
        "colab_type": "text"
      },
      "source": [
        "tensorflowのDatasetオブジェクトを作ろう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFQ6oDy1_QBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #24000\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE #375\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "#ボキャブラリ数 = 翻訳辞書の項目数であるため、inputもtargetも同じ値\n",
        "vocab_input_size = len(input_lang.word_index) + 1 #10060\n",
        "vocab_target_size = len(target_lang.word_index) + 1 #10060\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOnx0rA2_7uW",
        "colab_type": "code",
        "outputId": "fac6b25b-667e-4f17-9c40-adcfa00839e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ex_input_batch, ex_target_batch = next(iter(dataset))\n",
        "ex_input_batch.shape, ex_target_batch.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 27]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx8Nxj3aAJ7F",
        "colab_type": "text"
      },
      "source": [
        "これで、Attentionに渡すデータの準備ができた。\n",
        "\n",
        "## Encoder, Decoder\n",
        "\n",
        "machine translationの要である、encoderとdecoderを実装してみよう。\n",
        "\n",
        "まずはencoderからだ。 encoderの構成は以下の通り。\n",
        "\n",
        "* embedding\n",
        "* GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZqDlYed0pAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc49dNlX1y4N",
        "colab_type": "code",
        "outputId": "c1951124-761d-449e-d426-57d8d4fa9658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(ex_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 27, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9WJ5Slf3EEm",
        "colab_type": "text"
      },
      "source": [
        "次にattentionを実装する。\n",
        "\n",
        "attentionは、Luong attentionとBahdanau attentionの二種類が有名だ。両者の違いはattention scoreの計算方法である。前者はqueryとkeyの内積を計算する。後者はqueryとvalueの和を計算する。\n",
        "\n",
        "* Luong attention : $score(q, k) = k^{T}Wq$\n",
        "* Bahdanau attention : $score(q, k) = V^{T}tanh(W_1q + W_2v)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4CUjZC3Vy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    '''\n",
        "    scoreを計算する\n",
        "    '''\n",
        "    #query hidden state shape = (batch_size, hidden_size) = (64, 1024)\n",
        "    #quey_with_time_axis = (batch_size, 1, hidden_size) = (64, 1, 1024)\n",
        "    #values shape = (batch_size, max_len, hidden_size) = (64, 27, 1024)\n",
        "    #expand_dims()で、第二引数に指定したaxisの手前、つまりhidden_sizeの手前にサイズ１の次元を追加する。\n",
        "    #scoreを計算するためには、valuesのshapeとqueryのshapeを合致させる必要がある。\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    densed_q = self.W1(query_with_time_axis) #(64, 1, 10)\n",
        "    densed_v = self.W2(values) #(64, 27, 10)\n",
        "    tanh_q_v = tf.nn.tanh(densed_q + densed_v) #(64, 27, 10)\n",
        "    score = self.V(tanh_q_v) #(64, 27, 1)\n",
        "\n",
        "    '''\n",
        "    attention weightを計算する\n",
        "    axisを指定しないと最後の次元に対してsoftmaxを計算してしまう。\n",
        "    ここでは、max_length分だけの重みのsoftmaxを計算したいので、axis=1(番目の次元)を指定している\n",
        "    '''\n",
        "    attention_weights = tf.nn.softmax(score, axis=1) #(64, 27, 1)\n",
        "\n",
        "    '''\n",
        "    context_vectorを求める。\n",
        "    context_vector shape = (batch_size, hidden_size)\n",
        "    '''\n",
        "    context_vector = attention_weights * values #(64, 27, 1024)\n",
        "    #reduce_sumにより、axis=1次元目がなくなる\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) #(64, 1024)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roK8empo9Lop",
        "colab_type": "text"
      },
      "source": [
        "作成したattention layerの動作確認をしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWMNXMw8_iC",
        "colab_type": "code",
        "outputId": "35707d51-ae1d-4305-e05a-64cb4f87a652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch_size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 27, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8KpEtGlBSbZ",
        "colab_type": "text"
      },
      "source": [
        "次は、decoderを実装しよう。decoderの構成は以下の通り。\n",
        "\n",
        "* attention layer\n",
        "* embedding layer\n",
        "* GRU layer\n",
        "* FC layer\n",
        "\n",
        "通常のdecoderと異なる点は、decoderのembedding tensorにattention layerの出力tensorが加わる点だ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zbvlu3lBU3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True, \n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    '''\n",
        "    attention layer\n",
        "    '''\n",
        "    #context vector : (batch_size, hidden_size)\n",
        "    #attention weiths : (batch_size, sequence_length, 1)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    '''\n",
        "    embedding layer\n",
        "    '''\n",
        "    #(batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x) #(64, 1, 256)\n",
        "\n",
        "    #-1番目(最後)の次元の要素数を増やす\n",
        "    #(batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) #(64, 1, 1280)\n",
        "\n",
        "    '''\n",
        "    GRU layer\n",
        "    '''\n",
        "    #output: (64, 1, 1024)\n",
        "    #state: (64, 1024)\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2])) #(64, 1024)\n",
        "\n",
        "    #(batch_size, vocab)\n",
        "    predictions = self.fc(output)\n",
        "\n",
        "    return predictions, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XYc2OptGbQN",
        "colab_type": "text"
      },
      "source": [
        "Decoderの動作確認をしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31IYFDlsGeso",
        "colab_type": "code",
        "outputId": "40fa24bf-a3de-46dd-ff24-20ca8dbba962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "print('Decoder output shape: (batch_size, vocab_size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab_size) (64, 6113)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RF3i0iZIeZh",
        "colab_type": "text"
      },
      "source": [
        "### optimizerとloss functionを定義しよう\n",
        "\n",
        "optimizerはAdamを使う。loss objectにはSparseCategoricalCrossentropyを使い、出力はlogitとする。なお、loss functionとしては、出力のlogitsにmaskを掛け合わせたものの、平均値とする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24ZGxbeJT0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  #realが0でなければtrue, realが０であればfalse\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  #realが0ならlossの値も0になる\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQyyMwsnKa9Z",
        "colab_type": "text"
      },
      "source": [
        "## checkpointの定義\n",
        "\n",
        "checkpointを作っておこう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYQrt2hRKhJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyD4_qugKzfk",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "学習手順は以下の通り。\n",
        "\n",
        "1. 入力データをencodeする(encoderに渡す)。戻り値として、output, hidden state(GRUの出力値)を取得する\n",
        "\n",
        "2. encoder output, hidden state, decoder input(start token)をdecodeする。戻り値として、predictions, hidden stateを取得する。\n",
        "\n",
        "3. decoderのhidden stateをモデルに戻す。そして、predictionsを使ってloss functionsを計算する\n",
        "\n",
        "4. 「teacher」によって、target wordを次のdecoderのインプットとする。\n",
        "\n",
        "5. gradientを計算して、optimizerに適用、逆伝搬させる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKBd9DjQnXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input, target, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''\n",
        "    step1\n",
        "    '''\n",
        "    enc_output, enc_hidden = encoder(input, enc_hidden)\n",
        "\n",
        "    '''\n",
        "    step2-0\n",
        "    '''\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    #全てのステップtごとにloss functionを計算する。\n",
        "    for t in range(1, target.shape[1]):\n",
        "      '''\n",
        "      step2\n",
        "      '''\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      '''\n",
        "      step3\n",
        "      '''\n",
        "      loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "      '''\n",
        "      step4\n",
        "      '''\n",
        "      #teacher forcing\n",
        "      dec_input = tf.expand_dims(target[:, t], 1)\n",
        "  \n",
        "  #lossの平均値(batchごとのloss)\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  '''\n",
        "  step5\n",
        "  '''\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5PuGJaEU15f",
        "colab_type": "text"
      },
      "source": [
        "それでは実際に学習を走らせてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5GlU4TFU4tL",
        "colab_type": "code",
        "outputId": "1f094b52-49ce-4358-95a9-683fa7f5b439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(input, target, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4314b425dca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMJQn1ogx0W",
        "colab_type": "text"
      },
      "source": [
        "Google colabのTPUインスタンスだと半日がかり・・・😱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLq0BdeuJND_",
        "colab_type": "text"
      },
      "source": [
        "## Translate\n",
        "\n",
        "学習したモデルを使って、翻訳をしてみよう。\n",
        "\n",
        "* evaluate関数は、teacher forcingをしないこと以外はtraining関数と同じ処理をする。\n",
        "* <end> tokenを予測したらモデルは予測をストップする。\n",
        "* 時刻tごとにattention weightsを保存する。\n",
        "* encoder outputは一つの入力につき一度だけ計算されることに注意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv5Wa3s7KSTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):  \n",
        "  '''\n",
        "  入力文字列(翻訳対象)に対する前処理\n",
        "  '''\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  #['<start>', '今日', 'は', '良い', '天気', 'だ', '<end>']のようなトークン列が得られる\n",
        "  tokens = sentence.split(' ')\n",
        "  #tokenごとに、翻訳辞書内の何番目に存在するかを求める(stringをintegerにマッピングする)\n",
        "  inputs = [input_lang.word_index[i] for i in tokens]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], \n",
        "                                                         maxlen=max_length_input,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  '''\n",
        "  翻訳処理\n",
        "  '''\n",
        "  result = ''\n",
        "  \n",
        "  hidden = [tf.zeros((1, units))] #(1, 1024)\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_lang.word_index['<start>']], 0)\n",
        "\n",
        "  attntion_plot = np.zeros((max_length_target, max_length_input)) #(16, 27)\n",
        "\n",
        "  for t in range(max_length_target):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "    #一次元ベクトルに変形する\n",
        "    attention_weights = tf.reshape(attention_weights, (-1,)) #(1728,)\n",
        "    attention_plot[t] = attention_weights.numpy() \n",
        "\n",
        "    #翻訳後の単語インデックス\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy() #scalar(integer)\n",
        "    #翻訳後の単語\n",
        "    predicted_word = target_lang.index_word[prediction_id]\n",
        "\n",
        "    result += predicted_word + ' '\n",
        "\n",
        "    #最後まで翻訳したら終了\n",
        "    if predicted_word == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "    \n",
        "    dec_input = tf.expand_dims([predicted_id], 0) #(1, 1)\n",
        "  \n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YNAiBlUUZHC",
        "colab_type": "text"
      },
      "source": [
        "モデルの性能を評価するために、attention weightsをプロットしてみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOUfHyvgUaZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  #行列の中身をプロットする\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize':14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels(['' + predicted_sentence, fontdict=fontdict])\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MutipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MutipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xPUFGiQVegx",
        "colab_type": "text"
      },
      "source": [
        "translate関数を定義しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuNY0UvpViBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  #翻訳処理\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  #翻訳文の表示\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  #attention weightの表示\n",
        "  results = result.split(' ') #翻訳文の単語数\n",
        "  sentences = mecab_tokenize(sentence) #入力文の単語数\n",
        "  attention_plot = attentin_plot[:len(results), :len(sentences)]\n",
        "  plot_attentoin(attention_plot, sentences, results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP9DyI8eXSD4",
        "colab_type": "text"
      },
      "source": [
        "## 翻訳\n",
        "\n",
        "実際に翻訳してみよう!!\n",
        "\n",
        "最新の学習結果から、学習モデルをロードしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYg5lzQUXWP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4PdXi8NXoDZ",
        "colab_type": "text"
      },
      "source": [
        "#### 例１"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDBURw6TXq7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('今日はとても寒い。')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBbR6gpHX1MQ",
        "colab_type": "text"
      },
      "source": [
        "#### 例２"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6woG9crX3Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('これは私の人生だ。')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUNeigwlYALb",
        "colab_type": "text"
      },
      "source": [
        "#### 例３"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO5qJ_BqYB38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('あなたは、まだ家にいますか？')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytLpvIHhYPDg",
        "colab_type": "text"
      },
      "source": [
        "#### 例４"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfudAsINYHMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('それを理解してみよう。')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}