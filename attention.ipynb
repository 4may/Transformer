{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7S02AzLHu6nQpbD6CXd36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4may/Transformer/blob/master/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuXH2Q59dD60",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "2018年にGoogle Brainから発表された新しいNNアーキテクチャ。attentionを使った構造が肝になる。\n",
        "\n",
        "RNNと比較した時の、attentionのメリットは以下の通り。\n",
        "\n",
        "* 並列計算可能であるため、GPUの恩恵を受けやすい\n",
        "* データの時空間関係に依存せず、離れた位置にある情報同士の関係性も表現できる\n",
        "* RNNのようなループ構造をもたず、構造がシンプル\n",
        "\n",
        "逆に、デメリットは以下の通り。\n",
        "\n",
        "* 時系列データの場合、ある時刻tの出力を計算をするのに全ての時刻の入力を必要とするため、非効率な場合がある\n",
        "\n",
        "## Attention\n",
        "\n",
        "Transformerを学ぶ前に、先にAttentionを学んでおこう。ここでは、attentionを使って日本語を英語に翻訳してみよう。\n",
        "\n",
        "[Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n",
        "\n",
        "### 必要なモジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtsFeNggQLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzrXlQq4gwHb",
        "colab_type": "text"
      },
      "source": [
        "### データの準備\n",
        "\n",
        "データの前処理として、以下のことをしよう。\n",
        "\n",
        "1. 各文にstart/end tokenを付け加える。\n",
        "2. 各文から特殊文字を取り除く。\n",
        "3. 単語と、単語のindexの変換辞書を作る\n",
        "4. 各文をpaddingする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoSZuOR9tOWX",
        "colab_type": "text"
      },
      "source": [
        "### 形態素解析\n",
        "\n",
        "翻訳辞書内の日本語は分かち書きされていないため、形態素解析が必要。\n",
        "\n",
        "google colab上に、辞書と必要なモジュールをインストールする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgD03fFumUk",
        "colab_type": "code",
        "outputId": "98b2a5e2-d960-4bb4-c521-c609a1952dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.10-6ubuntu1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.3)\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.3)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (0.996.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3ICbYiuvYd",
        "colab_type": "code",
        "outputId": "69cb44b0-ebe1-4f95-c76b-0f595f57c4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mecab-ipadic-neologd' already exists and is not an empty directory.\n",
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : Check the existance of libraries\n",
            "[install-mecab-ipadic-NEologd] :     find => ok\n",
            "[install-mecab-ipadic-NEologd] :     sort => ok\n",
            "[install-mecab-ipadic-NEologd] :     head => ok\n",
            "[install-mecab-ipadic-NEologd] :     cut => ok\n",
            "[install-mecab-ipadic-NEologd] :     egrep => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab-config => ok\n",
            "[install-mecab-ipadic-NEologd] :     make => ok\n",
            "[install-mecab-ipadic-NEologd] :     curl => ok\n",
            "[install-mecab-ipadic-NEologd] :     sed => ok\n",
            "[install-mecab-ipadic-NEologd] :     cat => ok\n",
            "[install-mecab-ipadic-NEologd] :     diff => ok\n",
            "[install-mecab-ipadic-NEologd] :     tar => ok\n",
            "[install-mecab-ipadic-NEologd] :     unxz => ok\n",
            "[install-mecab-ipadic-NEologd] :     xargs => ok\n",
            "[install-mecab-ipadic-NEologd] :     grep => ok\n",
            "[install-mecab-ipadic-NEologd] :     iconv => ok\n",
            "[install-mecab-ipadic-NEologd] :     patch => ok\n",
            "[install-mecab-ipadic-NEologd] :     which => ok\n",
            "[install-mecab-ipadic-NEologd] :     file => ok\n",
            "[install-mecab-ipadic-NEologd] :     openssl => ok\n",
            "[install-mecab-ipadic-NEologd] :     awk => ok\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd is already up-to-date\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd will be install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Make mecab-ipadic-NEologd\n",
            "[make-mecab-ipadic-NEologd] : Start..\n",
            "[make-mecab-ipadic-NEologd] : Check local seed directory\n",
            "[make-mecab-ipadic-NEologd] : Check local seed file\n",
            "[make-mecab-ipadic-NEologd] : Check local build directory\n",
            "[make-mecab-ipadic-NEologd] : Download original mecab-ipadic file\n",
            "[make-mecab-ipadic-NEologd] : Original mecab-ipadic file is already there.\n",
            "[make-mecab-ipadic-NEologd] : Decompress original mecab-ipadic file\n",
            "[make-mecab-ipadic-NEologd] : Delete old mecab-ipadic-2.7.0-20070801-neologd-20200315 directory\n",
            "mecab-ipadic-2.7.0-20070801/\n",
            "mecab-ipadic-2.7.0-20070801/README\n",
            "mecab-ipadic-2.7.0-20070801/AUTHORS\n",
            "mecab-ipadic-2.7.0-20070801/COPYING\n",
            "mecab-ipadic-2.7.0-20070801/ChangeLog\n",
            "mecab-ipadic-2.7.0-20070801/INSTALL\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.am\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.in\n",
            "mecab-ipadic-2.7.0-20070801/NEWS\n",
            "mecab-ipadic-2.7.0-20070801/aclocal.m4\n",
            "mecab-ipadic-2.7.0-20070801/config.guess\n",
            "mecab-ipadic-2.7.0-20070801/config.sub\n",
            "mecab-ipadic-2.7.0-20070801/configure\n",
            "mecab-ipadic-2.7.0-20070801/configure.in\n",
            "mecab-ipadic-2.7.0-20070801/install-sh\n",
            "mecab-ipadic-2.7.0-20070801/missing\n",
            "mecab-ipadic-2.7.0-20070801/mkinstalldirs\n",
            "mecab-ipadic-2.7.0-20070801/Adj.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adnominal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adverb.csv\n",
            "mecab-ipadic-2.7.0-20070801/Auxil.csv\n",
            "mecab-ipadic-2.7.0-20070801/Conjunction.csv\n",
            "mecab-ipadic-2.7.0-20070801/Filler.csv\n",
            "mecab-ipadic-2.7.0-20070801/Interjection.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adjv.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.demonst.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.nai.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.name.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.number.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.org.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.place.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.proper.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.verbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp-col.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp.csv\n",
            "mecab-ipadic-2.7.0-20070801/Prefix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Suffix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Symbol.csv\n",
            "mecab-ipadic-2.7.0-20070801/Verb.csv\n",
            "mecab-ipadic-2.7.0-20070801/char.def\n",
            "mecab-ipadic-2.7.0-20070801/feature.def\n",
            "mecab-ipadic-2.7.0-20070801/left-id.def\n",
            "mecab-ipadic-2.7.0-20070801/matrix.def\n",
            "mecab-ipadic-2.7.0-20070801/pos-id.def\n",
            "mecab-ipadic-2.7.0-20070801/rewrite.def\n",
            "mecab-ipadic-2.7.0-20070801/right-id.def\n",
            "mecab-ipadic-2.7.0-20070801/unk.def\n",
            "mecab-ipadic-2.7.0-20070801/dicrc\n",
            "mecab-ipadic-2.7.0-20070801/RESULT\n",
            "[make-mecab-ipadic-NEologd] : Configure custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200315\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for working aclocal-1.4... missing\n",
            "checking for working autoconf... missing\n",
            "checking for working automake-1.4... missing\n",
            "checking for working autoheader... missing\n",
            "checking for working makeinfo... missing\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "[make-mecab-ipadic-NEologd] : Encode the character encoding of system dictionary resources from EUC_JP to UTF-8\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Auxil.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.number.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.place.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.proper.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adj.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Suffix.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adverb.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Filler.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.demonst.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp-col.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.verbal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.name.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Others.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Interjection.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adnominal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Symbol.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.nai.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adverbal.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adjv.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Verb.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Conjunction.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.others.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.org.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Prefix.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.csv \n",
            "rm ./Auxil.csv \n",
            "rm ./Noun.number.csv \n",
            "rm ./Noun.place.csv \n",
            "rm ./Noun.proper.csv \n",
            "rm ./Adj.csv \n",
            "rm ./Suffix.csv \n",
            "rm ./Adverb.csv \n",
            "rm ./Filler.csv \n",
            "rm ./Noun.demonst.csv \n",
            "rm ./Postp-col.csv \n",
            "rm ./Noun.verbal.csv \n",
            "rm ./Noun.name.csv \n",
            "rm ./Others.csv \n",
            "rm ./Postp.csv \n",
            "rm ./Interjection.csv \n",
            "rm ./Adnominal.csv \n",
            "rm ./Symbol.csv \n",
            "rm ./Noun.nai.csv \n",
            "rm ./Noun.adverbal.csv \n",
            "rm ./Noun.adjv.csv \n",
            "rm ./Verb.csv \n",
            "rm ./Conjunction.csv \n",
            "rm ./Noun.others.csv \n",
            "rm ./Noun.org.csv \n",
            "rm ./Prefix.csv \n",
            "rm ./Noun.csv \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./feature.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./char.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./matrix.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./unk.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./right-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./left-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./pos-id.def \n",
            "./../../libexec/iconv_euc_to_utf8.sh ./rewrite.def \n",
            "rm ./feature.def \n",
            "rm ./char.def \n",
            "rm ./matrix.def \n",
            "rm ./unk.def \n",
            "rm ./right-id.def \n",
            "rm ./left-id.def \n",
            "rm ./pos-id.def \n",
            "rm ./rewrite.def \n",
            "mv ./Noun.place.csv.utf8 ./Noun.place.csv \n",
            "mv ./matrix.def.utf8 ./matrix.def \n",
            "mv ./Noun.proper.csv.utf8 ./Noun.proper.csv \n",
            "mv ./Postp.csv.utf8 ./Postp.csv \n",
            "mv ./Suffix.csv.utf8 ./Suffix.csv \n",
            "mv ./Symbol.csv.utf8 ./Symbol.csv \n",
            "mv ./Prefix.csv.utf8 ./Prefix.csv \n",
            "mv ./Verb.csv.utf8 ./Verb.csv \n",
            "mv ./Noun.adjv.csv.utf8 ./Noun.adjv.csv \n",
            "mv ./Conjunction.csv.utf8 ./Conjunction.csv \n",
            "mv ./char.def.utf8 ./char.def \n",
            "mv ./Noun.adverbal.csv.utf8 ./Noun.adverbal.csv \n",
            "mv ./Postp-col.csv.utf8 ./Postp-col.csv \n",
            "mv ./right-id.def.utf8 ./right-id.def \n",
            "mv ./Noun.verbal.csv.utf8 ./Noun.verbal.csv \n",
            "mv ./unk.def.utf8 ./unk.def \n",
            "mv ./Noun.nai.csv.utf8 ./Noun.nai.csv \n",
            "mv ./Noun.name.csv.utf8 ./Noun.name.csv \n",
            "mv ./Noun.number.csv.utf8 ./Noun.number.csv \n",
            "mv ./feature.def.utf8 ./feature.def \n",
            "mv ./Adnominal.csv.utf8 ./Adnominal.csv \n",
            "mv ./Noun.others.csv.utf8 ./Noun.others.csv \n",
            "mv ./Interjection.csv.utf8 ./Interjection.csv \n",
            "mv ./Noun.demonst.csv.utf8 ./Noun.demonst.csv \n",
            "mv ./Adverb.csv.utf8 ./Adverb.csv \n",
            "mv ./Others.csv.utf8 ./Others.csv \n",
            "mv ./Auxil.csv.utf8 ./Auxil.csv \n",
            "mv ./rewrite.def.utf8 ./rewrite.def \n",
            "mv ./Filler.csv.utf8 ./Filler.csv \n",
            "mv ./Noun.org.csv.utf8 ./Noun.org.csv \n",
            "mv ./Adj.csv.utf8 ./Adj.csv \n",
            "mv ./left-id.def.utf8 ./left-id.def \n",
            "mv ./Noun.csv.utf8 ./Noun.csv \n",
            "mv ./pos-id.def.utf8 ./pos-id.def \n",
            "[make-mecab-ipadic-NEologd] : Fix yomigana field of IPA dictionary\n",
            "patching file Noun.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Verb.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.adverbal.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.others.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Prefix.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.demonst.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "[make-mecab-ipadic-NEologd] : Copy user dictionary resource\n",
            "[make-mecab-ipadic-NEologd] : Install adverb entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adverb-dict-seed.20150623.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install interjection entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-interjection-dict-seed.20170216.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-common-noun-ortho-variant-dict-seed.20170228.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-proper-noun-ortho-variant-dict-seed.20161110.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install entries of orthographic variant of a noun used as verb form using /content/mecab-ipadic-neologd/libexec/../seed/neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install frequent adjective orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-std-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent adjective orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-exp-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install adjective verb orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-verb-dict-seed.20160324.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent datetime representation entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-date-time-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install infrequent quantity representation entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-quantity-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install entries of ill formed words using /content/mecab-ipadic-neologd/libexec/../seed/neologd-ill-formed-words-dict-seed.20170127.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Re-Index system dictionary\n",
            "reading ./unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "reading ./neologd-adjective-std-dict-seed.20151126.csv ... 507812\n",
            "reading ./Auxil.csv ... 199\n",
            "reading ./Noun.number.csv ... 42\n",
            "reading ./Noun.place.csv ... 73194\n",
            "reading ./Noun.proper.csv ... 27493\n",
            "reading ./Adj.csv ... 27210\n",
            "reading ./Suffix.csv ... 1448\n",
            "reading ./neologd-quantity-infreq-dict-seed.20190415.csv ... 229216\n",
            "reading ./Adverb.csv ... 3032\n",
            "reading ./neologd-adjective-exp-dict-seed.20151126.csv ... 1051146\n",
            "reading ./neologd-common-noun-ortho-variant-dict-seed.20170228.csv ... 152869\n",
            "reading ./Filler.csv ... 19\n",
            "reading ./Noun.demonst.csv ... 120\n",
            "reading ./neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv ... 26058\n",
            "reading ./Postp-col.csv ... 91\n",
            "reading ./Noun.verbal.csv ... 12150\n",
            "reading ./neologd-adverb-dict-seed.20150623.csv ... 139792\n",
            "reading ./Noun.name.csv ... 34215\n",
            "reading ./Others.csv ... 2\n",
            "reading ./Postp.csv ... 146\n",
            "reading ./Interjection.csv ... 252\n",
            "reading ./neologd-interjection-dict-seed.20170216.csv ... 4701\n",
            "reading ./Adnominal.csv ... 135\n",
            "reading ./neologd-proper-noun-ortho-variant-dict-seed.20161110.csv ... 138379\n",
            "reading ./Symbol.csv ... 208\n",
            "reading ./Noun.nai.csv ... 42\n",
            "reading ./neologd-date-time-infreq-dict-seed.20190415.csv ... 16866\n",
            "reading ./Noun.adverbal.csv ... 808\n",
            "reading ./Noun.adjv.csv ... 3328\n",
            "reading ./neologd-ill-formed-words-dict-seed.20170127.csv ... 60616\n",
            "reading ./Verb.csv ... 130750\n",
            "reading ./Conjunction.csv ... 171\n",
            "reading ./Noun.others.csv ... 153\n",
            "reading ./Noun.org.csv ... 17149\n",
            "reading ./neologd-adjective-verb-dict-seed.20160324.csv ... 20268\n",
            "reading ./Prefix.csv ... 224\n",
            "reading ./mecab-user-dict-seed.20200315.csv ... 3187380\n",
            "reading ./Noun.csv ... 60734\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "[make-mecab-ipadic-NEologd] : Make custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200315\n",
            "make: Nothing to be done for 'all'.\n",
            "[make-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : Get results of tokenize test\n",
            "[test-mecab-ipadic-NEologd] : Start..\n",
            "[test-mecab-ipadic-NEologd] : Replace timestamp from 'git clone' date to 'git commit' date\n",
            "[test-mecab-ipadic-NEologd] : Get buzz phrases\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1405  100  1405    0     0   1446      0 --:--:-- --:--:-- --:--:--  1445\n",
            "[test-mecab-ipadic-NEologd] : Get difference between default system dictionary and mecab-ipadic-NEologd\n",
            "[test-mecab-ipadic-NEologd] : Tokenize phrase using default system dictionary\n",
            "[test-mecab-ipadic-NEologd] : Tokenize phrase using mecab-ipadic-NEologd\n",
            "[test-mecab-ipadic-NEologd] : Get result of diff\n",
            "[test-mecab-ipadic-NEologd] : Please check difference between default system dictionary and mecab-ipadic-NEologd\n",
            "\n",
            "default system dictionary\t  |\tmecab-ipadic-NEologd\n",
            "バーガー キング \t\t  |\tバーガーキング \n",
            "46 人 \t\t\t\t  |\t46人 \n",
            "オリジナル メンバー \t\t  |\tオリジナルメンバー \n",
            "スプラトゥーン 2 \t\t  |\tスプラトゥーン2 \n",
            "マリオ テニス \t\t\t  |\tマリオテニス \n",
            "東川 町 \t\t\t  |\t東川町 \n",
            "獣 神化 改 \t\t\t  |\t獣神化 改 \n",
            "アンドレ アル \t\t\t  |\tアンドレアル \n",
            "伊藤 静 \t\t\t  |\t伊藤静 \n",
            "道枝 駿 佑 \t\t\t  |\t道枝駿佑 \n",
            "小池 晃 \t\t\t  |\t小池晃 \n",
            "一 彩 \t\t\t\t  |\t一彩 \n",
            "\n",
            "[test-mecab-ipadic-NEologd] : Finish..\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Please check the list of differences in the upper part.\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Do you want to install mecab-ipadic-NEologd? Type yes or no.\n",
            "[install-mecab-ipadic-NEologd] : OK. Let's install mecab-ipadic-NEologd.\n",
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : /usr/lib/x86_64-linux-gnu/mecab/dic is current user's directory\n",
            "[install-mecab-ipadic-NEologd] : Make install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "make[1]: Entering directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200315'\n",
            "make[1]: Nothing to be done for 'install-exec-am'.\n",
            "/bin/bash ./mkinstalldirs /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            " /usr/bin/install -c -m 644 ./matrix.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/matrix.bin\n",
            " /usr/bin/install -c -m 644 ./char.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/char.bin\n",
            " /usr/bin/install -c -m 644 ./sys.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/sys.dic\n",
            " /usr/bin/install -c -m 644 ./unk.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/unk.dic\n",
            " /usr/bin/install -c -m 644 ./left-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/left-id.def\n",
            " /usr/bin/install -c -m 644 ./right-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/right-id.def\n",
            " /usr/bin/install -c -m 644 ./rewrite.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/rewrite.def\n",
            " /usr/bin/install -c -m 644 ./pos-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/pos-id.def\n",
            " /usr/bin/install -c -m 644 ./dicrc /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/dicrc\n",
            "make[1]: Leaving directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200315'\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Install completed.\n",
            "[install-mecab-ipadic-NEologd] : When you use MeCab, you can set '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd' as a value of '-d' option of MeCab.\n",
            "[install-mecab-ipadic-NEologd] : Usage of mecab-ipadic-NEologd is here.\n",
            "Usage:\n",
            "    $ mecab -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd ...\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKlOPxT2AiVy",
        "colab_type": "text"
      },
      "source": [
        "動作確認してみよう。print(\"input Language: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRE4aaTMva5n",
        "colab_type": "code",
        "outputId": "0f91bc69-fbfa-4232-c24a-3655ae0fe80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import MeCab\n",
        "import subprocess\n",
        "\n",
        "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
        "sub = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
        "path = (sub.communicate()[0]).decode('utf-8')\n",
        "m = MeCab.Tagger(\"-d {0}\".format(path))\n",
        "test = m.parse('今日は、良い気分になれる日だ！')\n",
        "print(test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "、\t記号,読点,*,*,*,*,、,、,、\n",
            "良い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,良い,ヨイ,ヨイ\n",
            "気分\t名詞,一般,*,*,*,*,気分,キブン,キブン\n",
            "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
            "なれる\t動詞,自立,*,*,一段,基本形,なれる,ナレル,ナレル\n",
            "日\t名詞,非自立,副詞可能,*,*,*,日,ヒ,ヒ\n",
            "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
            "！\t記号,一般,*,*,*,*,！,！,！\n",
            "EOS\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fF1ZlUExSFF",
        "colab_type": "code",
        "outputId": "1735dcb9-bb58-4bf0-da5d-7159c0c23e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lines = m.parse('今日は、良い気分になれる日だ！').split('\\n')\n",
        "#EOF, 空文字はスキップ\n",
        "tokens = [line.split('\\t')[0] for line in lines[:-2]]\n",
        "tokens"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['今日', 'は', '、', '良い', '気分', 'に', 'なれる', '日', 'だ', '！']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQifv_Tf3MjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mecab_tokenize(text):\n",
        "  lines = m.parse(text).split('\\n')\n",
        "  tokens = [line.split('\\t')[0] for line in lines[:-2]]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvPbQCH56FkE",
        "colab_type": "text"
      },
      "source": [
        "これで、分かち書きの準備ができた。\n",
        "\n",
        "#### 翻訳辞書をダウンロード\n",
        "\n",
        "[辞書のDL先](http://www.manythings.org/anki/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5MF6bjcqr4Q",
        "colab_type": "code",
        "outputId": "32f3933f-c32d-4f3c-994f-a8c71b350cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#ローカル辞書をアップロードする\n",
        "!unzip jpn-eng.zip"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  jpn-eng.zip\n",
            "replace jpn.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDA8ZkCVk6CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = 'jpn.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsIOlPcwm2Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  #日本語の場合、ユニコードカテゴリMnに該当する文字もnormalizeする必要がある。\n",
        "  #そうでなければ、濁点や丸点まで消えてしまう。\n",
        "  #return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s))\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  #形態素解析\n",
        "  tokens = mecab_tokenize(w)\n",
        "  w = ' '.join(tokens)\n",
        "\n",
        "  #小文字に変え、かつ文の先頭と末尾の空白と改行を削除する。\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  #単語と句読点との間にスペースを作る\n",
        "  #彼 は、少年 です。 -> 彼 は 、 少年 です 。\n",
        "  w = re.sub(r\"([?.!,、。])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  #<start> token、<end> tokenを付け加える\n",
        "  #モデルの予測の開始場所と終了場所がわかるようにする。\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vgOXC1Uo4WH",
        "colab_type": "code",
        "outputId": "5c973a7c-4e0b-4372-c25b-62666e21d0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "jp_sentence = u\"この本を、借りてもいいですか?\"\n",
        "preprocess_sentence(en_sentence)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> may i borrow this book ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owpfFWBFpKZZ",
        "colab_type": "code",
        "outputId": "47d22202-0ea3-48a1-a1d2-00445b14974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preprocess_sentence(jp_sentence)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> この 本 を 、 借りて も いい です か ? <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXCPwsgzpqUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  # 翻訳辞書の中身は以下のようになっている。\n",
        "  # 最後の注釈はいらないので、落とす。\n",
        "  # 英語\\t日本語\\t注釈\\n\n",
        "  # Go.\t行け。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421985 (Ninja)\n",
        "  # Go.\t行きなさい。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7421986 (Ninja)\n",
        "  # Hi.\tこんにちは。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #373351 (tommy_san)\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:-1]] for l in lines[:num_examples]]\n",
        "  \n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpp_Qi2hqVvG",
        "colab_type": "code",
        "outputId": "1b70c065-cf21-4831-cd35-f3fae8519167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "en, jp = create_dataset(path_to_file, None)\n",
        "en[-1]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> i do many things at the same time , so not only am i reading things by akutagawa , i've also increased the amount of time i spend reading in english and i also read a little in german every day . <end>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyjGSDQBsyrw",
        "colab_type": "code",
        "outputId": "36021ee1-3cc8-4aa8-c799-73c26c9f3630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "jp[-1]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> 色々 並行 し て やっ てる から 芥川 ばかり 読ん でる の で も ない の だ よ 。 今 は 英語 読ん でる 時間 が 増え てる 。 ドイツ語 も 毎日 少し ずつ やっ てる 。 <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIBvLcVz7mRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7do-AjJ58BIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  tartget_lang, input_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "  target_tensor, tartget_lang_tokenizer = tokenize(tartget_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, input_lang_tokenizer, tartget_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "068HEzi78bMK",
        "colab_type": "text"
      },
      "source": [
        "## 実験の高速化のため、データセットのサイズに制限を設ける\n",
        "\n",
        "100,000データだと多いので、30,000にしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6elHbSi8oYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "データ数を絞る\n",
        "'''\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, input_lang, target_lang = load_dataset(path_to_file, num_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4KCSnQJ9RRd",
        "colab_type": "code",
        "outputId": "19d254f9-6cf6-433a-8dc0-9b035934c9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "target tensorの最大長を取得\n",
        "'''\n",
        "max_length_target = target_tensor.shape[1]\n",
        "max_length_input = input_tensor.shape[1]\n",
        "max_length_target"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To4fqDMf9Wqm",
        "colab_type": "text"
      },
      "source": [
        "trainingデータとvalidationデータを8:2の割合で分けてみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OonyimFk9bsy",
        "colab_type": "code",
        "outputId": "20f35b1f-5800-4eb2-e814-1e49f05dccb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_tensor_train, input_tensor_valid, target_tensor_train, target_tensor_valid = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_valid), len(target_tensor_valid))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJi2IHI0-B77",
        "colab_type": "text"
      },
      "source": [
        "次に、単語と単語インデックスのマッピング関数を定義しよう。Attention layerは文字列ではなく数値を入力にとるため、文字列の数値変換が必要になる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDkF1fRp-NZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(\"%d ------> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOVg0Kg-ZaF",
        "colab_type": "code",
        "outputId": "e447fed7-bcc8-4e71-f627-24beb5505b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(\"Input Language: index to word mapping\")\n",
        "convert(input_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language: index to word mapping\")\n",
        "convert(target_lang, target_tensor_train[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language: index to word mapping\n",
            "1 ------> <start>\n",
            "275 ------> 大変\n",
            "212 ------> ありがとう\n",
            "446 ------> ござい\n",
            "33 ------> まし\n",
            "5 ------> た\n",
            "3 ------> 。\n",
            "2 ------> <end>\n",
            "\n",
            "Target Language: index to word mapping\n",
            "1 ------> <start>\n",
            "353 ------> thank\n",
            "7 ------> you\n",
            "96 ------> so\n",
            "349 ------> muc\n",
            "120 ------> h\n",
            "3 ------> .\n",
            "2 ------> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s27Qwkaa_Mnf",
        "colab_type": "text"
      },
      "source": [
        "tensorflowのDatasetオブジェクトを作ろう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFQ6oDy1_QBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train) #24000\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE #375\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "#ボキャブラリ数 = 翻訳辞書の項目数であるため、inputもtargetも同じ値\n",
        "vocab_input_size = len(input_lang.word_index) + 1 #10060\n",
        "vocab_target_size = len(target_lang.word_index) + 1 #10060\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOnx0rA2_7uW",
        "colab_type": "code",
        "outputId": "8600d150-7eb9-4ac9-f8c2-7ea02ee532de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ex_input_batch, ex_target_batch = next(iter(dataset))\n",
        "ex_input_batch.shape, ex_target_batch.shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 27]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx8Nxj3aAJ7F",
        "colab_type": "text"
      },
      "source": [
        "これで、Attentionに渡すデータの準備ができた。\n",
        "\n",
        "## Encoder, Decoder\n",
        "\n",
        "machine translationの要である、encoderとdecoderを実装してみよう。\n",
        "\n",
        "まずはencoderからだ。 encoderの構成は以下の通り。\n",
        "\n",
        "* embedding\n",
        "* GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZqDlYed0pAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc49dNlX1y4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30619b76-488f-4dda-c468-fa1aa3ae172b"
      },
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(ex_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 27, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9WJ5Slf3EEm",
        "colab_type": "text"
      },
      "source": [
        "次にattentionを実装する。\n",
        "\n",
        "attentionは、Luong attentionとBahdanau attentionの二種類が有名だ。両者の違いはattention scoreの計算方法である。前者はqueryとkeyの内積を計算する。後者はqueryとvalueの和を計算する。\n",
        "\n",
        "* Luong attention : $score(q, k) = k^{T}Wq$\n",
        "* Bahdanau attention : $score(q, k) = V^{T}tanh(W_1q + W_2v)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4CUjZC3Vy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    '''\n",
        "    scoreを計算する\n",
        "    '''\n",
        "    #query hidden state shape = (batch_size, hidden_size) = (64, 1024)\n",
        "    #quey_with_time_axis = (batch_size, 1, hidden_size) = (64, 1, 1024)\n",
        "    #values shape = (batch_size, max_len, hidden_size) = (64, 27, 1024)\n",
        "    #expand_dims()で、第二引数に指定したaxisの手前、つまりhidden_sizeの手前にサイズ１の次元を追加する。\n",
        "    #scoreを計算するためには、valuesのshapeとqueryのshapeを合致させる必要がある。\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    densed_q = self.W1(query_with_time_axis) #(64, 1, 10)\n",
        "    densed_v = self.W2(values) #(64, 27, 10)\n",
        "    tanh_q_v = tf.nn.tanh(densed_q + densed_v) #(64, 27, 10)\n",
        "    score = self.V(tanh_q_v) #(64, 27, 1)\n",
        "\n",
        "    '''\n",
        "    attention weightを計算する\n",
        "    axisを指定しないと最後の次元に対してsoftmaxを計算してしまう。\n",
        "    ここでは、max_length分だけの重みのsoftmaxを計算したいので、axis=1(番目の次元)を指定している\n",
        "    '''\n",
        "    attention_weights = tf.nn.softmax(score, axis=1) #(64, 27, 1)\n",
        "\n",
        "    '''\n",
        "    context_vectorを求める。\n",
        "    context_vector shape = (batch_size, hidden_size)\n",
        "    '''\n",
        "    context_vector = attention_weights * values #(64, 27, 1024)\n",
        "    #reduce_sumにより、axis=1次元目がなくなる\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) #(64, 1024)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roK8empo9Lop",
        "colab_type": "text"
      },
      "source": [
        "作成したattention layerの動作確認をしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWMNXMw8_iC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42d14502-3a08-4dbf-db89-c2820dc7c1fe"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch_size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 27, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8KpEtGlBSbZ",
        "colab_type": "text"
      },
      "source": [
        "次は、decoderを実装しよう。decoderの構成は以下の通り。\n",
        "\n",
        "* attention layer\n",
        "* embedding layer\n",
        "* GRU layer\n",
        "* FC layer\n",
        "\n",
        "通常のdecoderと異なる点は、decoderのembedding tensorにattention layerの出力tensorが加わる点だ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zbvlu3lBU3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True, \n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    '''\n",
        "    attention layer\n",
        "    '''\n",
        "    #context vector : (batch_size, hidden_size)\n",
        "    #attention weiths : (batch_size, sequence_length, 1)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    '''\n",
        "    embedding layer\n",
        "    '''\n",
        "    #(batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x) #(64, 1, 256)\n",
        "\n",
        "    #-1番目(最後)の次元の要素数を増やす\n",
        "    #(batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) #(64, 1, 1280)\n",
        "\n",
        "    '''\n",
        "    GRU layer\n",
        "    '''\n",
        "    #output: (64, 1, 1024)\n",
        "    #state: (64, 1024)\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2])) #(64, 1024)\n",
        "\n",
        "    #(batch_size, vocab)\n",
        "    predictions = self.fc(output)\n",
        "\n",
        "    return predictions, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XYc2OptGbQN",
        "colab_type": "text"
      },
      "source": [
        "Decoderの動作確認をしよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31IYFDlsGeso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bf748bf-9c58-4d26-a372-1f0db1db2545"
      },
      "source": [
        "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "print('Decoder output shape: (batch_size, vocab_size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab_size) (64, 6113)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RF3i0iZIeZh",
        "colab_type": "text"
      },
      "source": [
        "### optimizerとloss functionを定義しよう\n",
        "\n",
        "optimizerはAdamを使う。loss objectにはSparseCategoricalCrossentropyを使い、出力はlogitとする。なお、loss functionとしては、出力のlogitsにmaskを掛け合わせたものの、平均値とする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24ZGxbeJT0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  #realが0でなければtrue, realが０であればfalse\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  #realが0ならlossの値も0になる\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQyyMwsnKa9Z",
        "colab_type": "text"
      },
      "source": [
        "## checkpointの定義\n",
        "\n",
        "checkpointを作っておこう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYQrt2hRKhJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyD4_qugKzfk",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "学習手順は以下の通り。\n",
        "\n",
        "1. 入力データをencodeする(encoderに渡す)。戻り値として、output, hidden state(GRUの出力値)を取得する\n",
        "\n",
        "2. encoder output, hidden state, decoder input(start token)をdecodeする。戻り値として、predictions, hidden stateを取得する。\n",
        "\n",
        "3. decoderのhidden stateをモデルに戻す。そして、predictionsを使ってloss functionsを計算する\n",
        "\n",
        "4. 「teacher」によって、target wordを次のdecoderのインプットとする。\n",
        "\n",
        "5. gradientを計算して、optimizerに適用、逆伝搬させる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKBd9DjQnXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input, target, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''\n",
        "    step1\n",
        "    '''\n",
        "    enc_output, enc_hidden = encoder(input, enc_hidden)\n",
        "\n",
        "    '''\n",
        "    step2-0\n",
        "    '''\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    #全てのステップtごとにloss functionを計算する。\n",
        "    for t in range(1, target.shape[1]):\n",
        "      '''\n",
        "      step2\n",
        "      '''\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      '''\n",
        "      step3\n",
        "      '''\n",
        "      loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "      '''\n",
        "      step4\n",
        "      '''\n",
        "      #teacher forcing\n",
        "      dec_input = tf.expand_dims(target[:, t], 1)\n",
        "  \n",
        "  #lossの平均値(batchごとのloss)\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  '''\n",
        "  step5\n",
        "  '''\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5PuGJaEU15f",
        "colab_type": "text"
      },
      "source": [
        "それでは実際に学習を走らせてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5GlU4TFU4tL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "1929a1c9-0dcd-420f-f56a-4fe09300d6cb"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(input, target, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Epoch {} Loss {:.4f}'.fomat(epoch+1, total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.tima() - start))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.0781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-5eb41410e3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}